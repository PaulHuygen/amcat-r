Vocabulary Analysis and LDA using the AmCAT API
===============================================

The AmCAT API has an endpoint for downloading token frequencies per document.
This allows for corpus linguistic analyses such as frequency analysis, collocations, 
corpus comparisons, and topic modeling. 

When downloading token frequencies, one usually wants lemmatized and/or POS-tagged tokens.
This means that the word 'means' is analysed as a verb (POS) with lemma mean.
Technically, this linguistic processing is performed by AmCAT using [xTas](www.xtas.net), 
which means that the first time a document is requested with a certain analysis it is cached,
so asking the same document again does not cause processing to be repeated.

Requesting tokens
-----------------

Tokens are requested from the `projects/articleset/ID/tokens` endpoint, 
specifying the set and analysis module to use. 
Currently, the useful modules are `corenlp_lemmatize` for English (default)
and `tadpole` for Dutch. 

*Note*: As of April 2014, the vocabulary analysis features are not yet available through 
the production version of amcat (release 3.3). 
However, [preview.amcat.nl](http://preview.amcat.nl)
gives access to the same database using the newest development version of AmCAT.
Therefore, this howto connects to that server rather than the default server:

```{r message=FALSE}
library(amcatr)
conn = amcat.connect("https://amcat.nl")
t = amcat.gettokens(conn, project=442, articleset=10271, module="corenlp_lemmatize",
                    page_size=1, npages=1)              
tail(t, n=10)
```

The command displayed above requests the tokens for a single article by setting `page_size` to 1 and requesting only a single page.
In the output you can see that it gives the frequency per word per article, and gives lemma and pos information. 
For example, the last word `your` has lemma `you` and POS possessive pronoun (`PRP$`). 
The `pos1` column gives a simplified POS tag, with `N` for nouns, `V` for verbs, `A` for adjectives, and `M` for proper names. 

If we want to get the term frequencies for a larger set of articles, 
it can be useful to select only the data we really need. 
The following command requests all tokens for the publicly available set 10271 
containing wikinews articles on Iraq.
It filters on 'substantive' POS tags N (noun), M (name), V (verb), and A (adjective), 
and only downloads the lemma per article:

```{r message=FALSE}
t = amcat.gettokens(conn, project=442, articleset=10271, module="corenlp_lemmatize", 
                    page_size=100, keep=c("aid", "lemma"),
                    filters=c(pos1="N", pos1="M", pos1="V", pos1="M"))
head(t, n=10)
```

Term-document matrix
--------------------

The feature list (or actually: feature data frame) created above can be analysed directly,
for example by listing the most frequent words:

```{r}
wordfreqs = aggregate(t$freq, by=list(word=t$lemma), FUN=sum)
wordfreqs = wordfreqs[order(-wordfreqs$x), ]
head(wordfreqs)
```

However, for most purposes it is better to first create a document-term matrix of the standard form defined in the `tm` package:

```{r}
dtm = amcat.dtm.create(t$aid, t$lemma, t$freq)
```

Since the resulting document-term matrix object is defined in the widely used `tm` package, 
a large number of existing packages and functions can be used to analyse it. 

Term Frequency
--------------

To make it easy to select words on term frequency and/or inverse document frequency,
we created a function to calculate a number of useful term statistics on a dtm object:

```{r}
terms = amcat.term.statistics(dtm)
head(terms, n=10)
```

For every term, the term frequency, document frequency, and tf-idf is given. 
Moreover, the number of characters in the word is given and whether the word contains a number or non-alphanumeric character.
These latter variables are useful to filter out nonsense terms, such as the %-sign in the list above.
For example, the following selects all words with at least three characters, not containing a number of symbol and 
with a term frequency of at least 10, and sorts them by td-idf:

```{r}
voca = terms[!terms$number & !terms$nonalpha & terms$termfreq > 10 & terms$characters >= 3, ]
voca = voca[order(voca$tfidf), ]
head(voca, n=10)
```

These terms are presumably the more 'informative' terms in the vocabulary.

Comparing corpora
-----------------

It can be interesting to compare vocabulary use in two corpora. 
For example, lets split our corpus into those articles that mention Bush and those that do not:

```{r}
ncol(dtm)
w = as.matrix(dtm[,"Bush"])
dtm.bush = dtm[w>0, ]
dtm.rest = dtm[w==0, ]
```


Now, we can compute and compare the term frequencies for both:

```{r}
terms.bush = amcat.term.statistics(dtm.bush)
terms.rest = amcat.term.statistics(dtm.rest)
freqs.rest = terms.rest[, c("term", "termfreq")]
terms.bush = merge(terms.bush, freqs.rest, all.x=TRUE, by="term")
terms.bush[is.na(terms.bush)] = 0
head(terms.bush)
```

`terms.bush` now contains the term statistics in the articles mentioning bush, 
as well as the frequency in the reference corpus consisting of the other articles.
We can now compute and sort by the overrepresentation of terms:

```{r}
terms.bush$relfreq.x = terms.bush$termfreq.x / sum(terms.bush$termfreq.x)
terms.bush$relfreq.y = terms.bush$termfreq.y / sum(terms.rest$termfreq)
terms.bush$over = terms.bush$relfreq.x / (terms.bush$relfreq.y + .001)
terms.bush = terms.bush[order(-terms.bush$over), ]
head(terms.bush, n=10)
```

This gives a list of the words that occur 'too much' in the articles mentioning Bush,
or in other words the collocates of the word 'Bush'.
To make this easier, and to also provide statistical association measures such as chi-squared,
the function `amcat.compare.corpora` is provided. 
The following example selects all words that are underrepresented in the `bush` corpus,
and sorts them by chi-squared:

```{r}
terms = amcat.compare.corpora(dtm.bush, dtm.rest)
terms = terms[terms$over < 1, ]
terms = terms[order(-terms$chi),]
head(terms)
```

What can be seen from these two word lists is that the articles mentioning Bush are more political in nature,
while the other articles describe more (military) action. 

Of course, this can also be used to compare e.g. vocabulary differences between newspapers, speakers, periods, etc.
For example, the following uses the article metadata to compare vocabulary after 2012 with the vocabulary before that date.
Note that `rownames(dtm)` contains the article ids (as a character vector),
so by matching the ids in the meta file with the (numeric) row names we can put the years in the same order as the rows of `dtm`.

```{r message=FALSE}
meta = amcat.getarticlemeta(conn, set=10271, dateparts=T)
years = meta$year[match(as.numeric(rownames(dtm)), meta$id)]
dtm.before = dtm[years < as.Date('2012-01-01'),]
dtm.after = dtm[years >= as.Date('2012-01-01'),]
terms = amcat.compare.corpora(dtm.after, dtm.before)
terms = terms[order(-terms$chi),]
head(terms[terms$over > 1, ])
head(terms[terms$over < 1, ])
```

So, the later articles mention the scandals with Abu Ghraib and phosphorus munition more frequently, 
while the earlier articles metion the constitution and refugees. 

Topic modeling
--------------

An interesting technique to use on a document-term matrix is that of LDA topic modeling using the r `lda` package. 
Topic modeling essentially reduces the dimensionality of the word space 
by assuming that each document contains a number of (latent) topics, which in turn contain a number of words (Blei et al, JMLR 2003). 

Before fitting the topic model, it is best to reduce the vocabulary by selecting only informative words.
Then, we can fit a topic model using the `amcatr.lda.fit` function, which requires a dtm object and the number of topics:

```{r}
terms = amcat.term.statistics(dtm)
voca = terms[!terms$number & !terms$nonalpha & terms$termfreq > 10 & terms$characters >= 3 & terms$tfidf > .05, ]
dtm = dtm[, colnames(dtm) %in% voca$term]
m = amcat.lda.fit(dtm, 5)
```

The `m` object is the standard object returned by the `lda` package, 
which also provides functions for inspecting its contents:

```{r}
top.topic.words(m$topics)
```

A final function provided by the `amcatr` package is the `amcat.lda.topics.per.document`,
which gives a data frame containing all articles and the topics occurring in each article.
By merging this with the metadata we can see which articles contained each topic:

```{r message=FALSE}
docs = amcat.lda.topics.per.document(m)
meta = amcat.getarticlemeta(conn, set=10271, dateparts=T)
docs = merge(meta, docs, all.y=T)
head(docs)
```

And we can use this to e.g. plot the topic use over time:

```{r lda-plot}
topics.per.year = aggregate(docs[, c("X1","X2","X3","X4","X5")], by=list(year=docs$year), FUN=sum)
topics.per.year$total = rowSums(topics.per.year[-1])
rel= topics.per.year[2:6] / topics.per.year$total

plot(topics.per.year$year, topics.per.year$X1, type="n", ylim=c(min(rel), max(rel)),
     ylab="Topic frequency",xlab="Year", frame.plot=F)
for(i in 1:5) 
  lines(topics.per.year$year, rel[, i], col=rainbow(5)[i])
legend("topleft", legend=colnames(rel), lwd=2, col=rainbow(5))
```
